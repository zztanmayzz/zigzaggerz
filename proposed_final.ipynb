{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQSNOthoLlnGhwj+syRvQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zztanmayzz/zigzaggerz/blob/main/proposed_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from pathlib import Path\n",
        "\n",
        "class FloodSegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for flood management semantic segmentation\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n",
        "        self.image_dir = Path(image_dir)\n",
        "        self.mask_dir = Path(mask_dir)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        # Get all image files\n",
        "        self.images = list(self.image_dir.glob(\"*.jpg\")) + list(self.image_dir.glob(\"*.png\"))\n",
        "\n",
        "        # Class mapping\n",
        "        self.class_colors = {\n",
        "            (255, 0, 0): 1,    # Buildings - red\n",
        "            (0, 255, 0): 2,    # Vegetation - green\n",
        "            (128, 128, 128): 3, # Roads - grey\n",
        "            (139, 69, 19): 4,  # Bare ground - brown\n",
        "            (255, 255, 255): 0 # Background - white\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def rgb_to_class_mask(self, rgb_mask):\n",
        "        \"\"\"Convert RGB mask to class indices\"\"\"\n",
        "        h, w = rgb_mask.shape[:2]\n",
        "        class_mask = np.zeros((h, w), dtype=np.long)\n",
        "\n",
        "        for color, class_id in self.class_colors.items():\n",
        "            mask = np.all(rgb_mask == color, axis=-1)\n",
        "            class_mask[mask] = class_id\n",
        "\n",
        "        return class_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Load corresponding mask (same name, different directory)\n",
        "        mask_path = self.mask_dir / img_path.name\n",
        "        mask = Image.open(mask_path).convert('RGB')\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Convert RGB mask to class indices\n",
        "        mask = self.rgb_to_class_mask(mask)\n",
        "        mask = Image.fromarray(mask.astype(np.uint8))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def create_data_loaders(train_dir, mask_dir, batch_size=4, img_size=224):\n",
        "    \"\"\"Create training and validation data loaders\"\"\"\n",
        "\n",
        "    # Data transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    mask_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = FloodSegmentationDataset(\n",
        "        train_dir, mask_dir,\n",
        "        transform=train_transform,\n",
        "        target_transform=mask_transform\n",
        "    )\n",
        "\n",
        "    # Split dataset (80% train, 20% val)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Example usage\n",
        "def setup_dataset_structure():\n",
        "    \"\"\"\n",
        "    Create the required directory structure\n",
        "    \"\"\"\n",
        "    directories = [\n",
        "        \"data/satellite_images\",    # Original satellite images\n",
        "        \"data/annotations\",         # Your corrected segmentation masks\n",
        "        \"data/elevation\",           # DEM data\n",
        "        \"data/soil\",                # Soil properties\n",
        "        \"models\",                   # Trained models\n",
        "        \"outputs\"                   # Results and predictions\n",
        "    ]\n",
        "\n",
        "    for dir_path in directories:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "        print(f\"Created directory: {dir_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_dataset_structure()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXIqjUqiIX1N",
        "outputId": "39061255-ed69-46d6-889a-77f7b2687dfa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: data/satellite_images\n",
            "Created directory: data/annotations\n",
            "Created directory: data/elevation\n",
            "Created directory: data/soil\n",
            "Created directory: models\n",
            "Created directory: outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!cp -r ./image{0..10}.png ./data/satellite_images/\n",
        "!touch ./data/annotations/image{0..10}.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6VnxKnoItq6",
        "outputId": "5fd57bfd-a2cc-4846-fb09-fb7fe32d1e55"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " data\t       image3.png\t image6.png\t   image9.png\n",
            " image10.png   image4.png\t image7.png\t   models\n",
            " image1.png    image5.png\t'image8 (1).png'   outputs\n",
            " image2.png   'image6 (1).png'\t image8.png\t   sample_data\n",
            "cp: cannot stat './image0.png': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ],
      "metadata": {
        "id": "HU5TRZlQJ77R"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_color_values(image_path, output_path):\n",
        "    \"\"\"\n",
        "    Fix the color values in annotated images to match exact legend values\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')  # Force convert to RGB\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Color mapping for correction\n",
        "    color_corrections = {\n",
        "        # Current -> Target\n",
        "        (254, 0, 0): (255, 0, 0),      # Buildings: red\n",
        "        (1, 255, 0): (0, 255, 0),      # Vegetation: green\n",
        "        (255, 176, 49): (139, 69, 19), # Convert orange areas to bare ground: brown\n",
        "    }\n",
        "\n",
        "    # Apply color corrections\n",
        "    for old_color, new_color in color_corrections.items():\n",
        "        mask = np.all(img_array == old_color, axis=-1)\n",
        "        img_array[mask] = new_color\n",
        "\n",
        "    # Save corrected image\n",
        "    corrected_img = Image.fromarray(img_array)\n",
        "    corrected_img.save(output_path)\n",
        "    print(f\"Corrected image saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "AuVLko1MLi5l"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "#from dataset_preparation import FloodSegmentationDataset, create_data_loaders\n",
        "\n",
        "class PrithviFloodModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Prithvi-based model for flood management segmentation\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=5, pretrained_path=None):\n",
        "        super(PrithviFloodModel, self).__init__()\n",
        "\n",
        "        # Download Prithvi model if not available\n",
        "        if pretrained_path is None:\n",
        "            try:\n",
        "                model_path = hf_hub_download(\n",
        "                    repo_id=\"ibm-nasa-geospatial/Prithvi-EO-1.0-100M\",\n",
        "                    filename=\"Prithvi_100M.pt\"\n",
        "                )\n",
        "                config_path = hf_hub_download(\n",
        "                    repo_id=\"ibm-nasa-geospatial/Prithvi-EO-1.0-100M\",\n",
        "                    filename=\"Prithvi_100M_config.yaml\"\n",
        "                )\n",
        "            except:\n",
        "                print(\"Warning: Could not download Prithvi model. Using alternative approach.\")\n",
        "                model_path = None\n",
        "                config_path = None\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # For hackathon speed, use a simpler segmentation model\n",
        "        # Replace with actual Prithvi when available\n",
        "        from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "        self.model = deeplabv3_resnet50(pretrained=True)\n",
        "\n",
        "        # Modify classifier for our classes\n",
        "        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)['out']\n",
        "\n",
        "def calculate_iou(pred, target, num_classes):\n",
        "    \"\"\"Calculate Intersection over Union for each class\"\"\"\n",
        "    ious = []\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls)\n",
        "        target_cls = (target == cls)\n",
        "\n",
        "        intersection = (pred_cls & target_cls).sum().float()\n",
        "        union = (pred_cls | target_cls).sum().float()\n",
        "\n",
        "        if union == 0:\n",
        "            iou = 1.0  # Perfect score for classes not present\n",
        "        else:\n",
        "            iou = intersection / union\n",
        "        ious.append(iou.item())\n",
        "\n",
        "    return ious\n",
        "\n",
        "def train_model(train_loader, val_loader, num_epochs=50, learning_rate=1e-4):\n",
        "    \"\"\"Train the flood segmentation model\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = PrithviFloodModel(num_classes=5)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore unknown pixels\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    best_val_iou = 0.0\n",
        "    train_losses = []\n",
        "    val_ious = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).long().squeeze(1)  # Remove channel dimension\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_iou_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device).long().squeeze(1)\n",
        "\n",
        "                outputs = model(images)\n",
        "                ious = calculate_iou(outputs, masks, num_classes=5)\n",
        "                val_iou_scores.append(ious)\n",
        "\n",
        "        # Calculate average IoU\n",
        "        mean_ious = np.mean(val_iou_scores, axis=0)\n",
        "        overall_iou = np.mean(mean_ious[1:])  # Exclude background class\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        val_ious.append(overall_iou)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {train_losses[-1]:.4f}')\n",
        "        print(f'  Val IoU: {overall_iou:.4f}')\n",
        "        print(f'  Class IoUs: Background={mean_ious[0]:.3f}, Buildings={mean_ious[1]:.3f}, '\n",
        "              f'Vegetation={mean_ious[2]:.3f}, Roads={mean_ious[3]:.3f}, Bare={mean_ious[4]:.3f}')\n",
        "\n",
        "        # Save best model\n",
        "        if overall_iou > best_val_iou:\n",
        "            best_val_iou = overall_iou\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_iou': overall_iou,\n",
        "                'class_ious': mean_ious\n",
        "            }, 'models/best_flood_model.pth')\n",
        "            print(f'  New best model saved with IoU: {overall_iou:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "        print('-' * 50)\n",
        "\n",
        "    return model, train_losses, val_ious\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    print(\"Starting Prithvi-based flood segmentation model training...\")\n",
        "\n",
        "    # Create data loaders\n",
        "    try:\n",
        "        train_loader, val_loader = create_data_loaders(\n",
        "            train_dir=\"data/satellite_images\",\n",
        "            mask_dir=\"data/annotations\",\n",
        "            batch_size=4,\n",
        "            img_size=224\n",
        "        )\n",
        "\n",
        "        print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "        print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "\n",
        "        # Train model\n",
        "        model, train_losses, val_ious = train_model(\n",
        "            train_loader, val_loader,\n",
        "            num_epochs=100,  # Increase for better results\n",
        "            learning_rate=1e-4\n",
        "        )\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        print(f\"Best validation IoU: {max(val_ious):.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        print(\"Make sure you have:\")\n",
        "        print(\"1. Fixed and placed images in data/satellite_images/\")\n",
        "        print(\"2. Corrected annotations in data/annotations/\")\n",
        "        print(\"3. All 4 classes present in annotations\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9G_XCz2M07g",
        "outputId": "f667c9ba-19af-4021-9238-a7e4067e3350"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Prithvi-based flood segmentation model training...\n",
            "Training samples: 10\n",
            "Validation samples: 3\n",
            "Training on device: cpu\n",
            "Warning: Could not download Prithvi model. Using alternative approach.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during training: cannot identify image file 'data/annotations/image2.png'\n",
            "Make sure you have:\n",
            "1. Fixed and placed images in data/satellite_images/\n",
            "2. Corrected annotations in data/annotations/\n",
            "3. All 4 classes present in annotations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C9hPX2ARxQt",
        "outputId": "43fa62c4-e873-4cf1-cdae-c3a74a25c4d7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " data\t       image3.png\t image6.png\t   image9.png\n",
            " image10.png   image4.png\t image7.png\t   models\n",
            " image1.png    image5.png\t'image8 (1).png'   outputs\n",
            " image2.png   'image6 (1).png'\t image8.png\t   sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gq0fG8HhSCd8"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}